### BERT

#### BERT框架的两大步骤：预训练和微调

1. 预训练：模型在不同的预训练任务中基于未标记数据进行训练
2. 微调：首先使用预训练参数初始化BERT模型，然后使用来自下游任务的标记数据微调所有参数

#### 参数声明：

- Transformer层数：L
- 隐层宽度：H
- 多头注意力机制头数：A

#### 输入输出表示法

为了使BERT可以解决多种类的下游任务，其输入可以在一个token序列之中表示单个句子或是句子对

序列的开头为[CLS]标记，使用[SEP]标记将句子对分开，其次向每一个token添加embedding，从而只是其属于句子A还是B

同时还包括了经典transformer的Position编码

<img src="https://yzx-drawing-bed.oss-cn-hangzhou.aliyuncs.com/img/202209062024473.png" alt="image-20220906202419423" style="zoom: 67%;" />

#### 预训练

BERT不适用传统的从左到右或者从右到左的语言模型来预训练BERT。使用的是两个无监督任务来进行预训练：

**任务一：Masked LM**

为了训练深度双向表示，只需随机屏蔽某些百分比的输入标记，然后预测这些屏蔽标记。我们将这一过程称为蒙蔽LM（MLM）。而为了解决在预训练和微调过程之中的不匹配问题，我们并不总是将被掩盖的词变为[MASK]标记

训练数据生成器随机选择15%的令牌位置用于预测。如果选择第i个令牌，我们将第i个替换为（1）80%概率替换为[MASK]（2）10%概率替换为随机（3）10%的概率不改变

![image-20220906211817656](https://yzx-drawing-bed.oss-cn-hangzhou.aliyuncs.com/img/202209062118684.png)

**任务二：Next Sentence Prediction  (NSP)**

许多重要的下游任务，如问答（QA）和自然语言推理（NLI），都是基于理解两个句子之间的关系。为了训练一个理解句子关系的模型，我们对一个二值化的下一个句子预测任务进行了预训练，该任务可以从任何单语语料库中简单地生成。

#### 微调

在微调部分之中，只需要将任务特有的输入和输出对送入BERT模型，并且端到端地微调所有的参数。

输入端可能有：（1）句子对（2）蕴含中的假设-前提对（3）问题-答案对（4）退化的text-空集对（在分类或者序列标注任务中）

输出端：token在序列标注或者问答任务中被送入输出层，而在情感分析等任务则是[CLS]表示被送入输出层

